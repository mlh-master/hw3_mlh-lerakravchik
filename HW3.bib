Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{svm,
title = {{SUPPORT VECTOR MACHINES(SVM). Introduction: All you need to know{\ldots} | by Ajay Yadav | Towards Data Science}},
url = {https://towardsdatascience.com/support-vector-machines-svm-c9ef22815589},
urldate = {2021-01-13}
}
@article{Lloyd,
abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 2b quanta, b = 1,2, {\textperiodcentered}{\textperiodcentered}{\textperiodcentered}, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes. {\textcopyright}1982 IEEE},
author = {Lloyd, Stuart P.},
doi = {10.1109/TIT.1982.1056489},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {2},
pages = {129--137},
title = {{Least Squares Quantization in PCM}},
volume = {28},
year = {1982}
}
@book{Kaufman,
address = {Hoboken, NJ, USA},
doi = {10.1002/9780470316801},
editor = {Kaufman, Leonard and Rousseeuw, Peter J.},
isbn = {9780470316801},
month = {mar},
publisher = {John Wiley {\&} Sons, Inc.},
series = {Wiley Series in Probability and Statistics},
title = {{Finding Groups in Data}},
url = {http://doi.wiley.com/10.1002/9780470316801},
year = {1990}
}
@incollection{Mannor2011,
abstract = {Synonyms Multi-armed bandit; Multi-armed bandit problem Definition In the classical k-armed bandit problem, there are k alternative arms, each with a stochastic reward whose probability distribution is initially unknown. A decision maker can try these arms in some order, which may depend on the rewards that have been observed so far. A common objective in this context is to ond a policy for choosing the next arm to be tried, under which the sum of the expected rewards comes as close as possible to the ideal reward, that is, the expected reward that would be obtained if it were to try the "best" arm at all times. ere are many variants of the k-armed bandit problem that are distinguished by the objective of the decision maker, the process governing the reward of each arm, and the information available to the decision maker at the end of every trial.},
author = {Mannor, Shie and Jin, Xin and Han, Jiawei and Jin, Xin and Han, Jiawei and Jin, Xin and Han, Jiawei and Zhang, Xinhua},
booktitle = {Encyclopedia of Machine Learning},
doi = {10.1007/978-0-387-30164-8_426},
file = {:C$\backslash$:/Users/Valeriia Kravchik/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mannor et al. - 2011 - K-Medoids Clustering.pdf:pdf},
pages = {564--565},
publisher = {Springer US},
title = {{K-Medoids Clustering}},
url = {https://link.springer.com/referenceworkentry/10.1007/978-0-387-30164-8{\_}426},
year = {2011}
}
